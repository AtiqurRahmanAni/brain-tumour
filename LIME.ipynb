{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "import torch_directml\n",
    "import matplotlib.image as imgs\n",
    "from transformers import BeitImageProcessor, BeitModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dml = torch_directml.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BEiTLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BEiTLSTM, self).__init__()\n",
    "        self.beit_model = BeitModel.from_pretrained(\"microsoft/beit-base-patch16-224\")\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size= hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Sequential(nn.Linear(self.hidden_size, 150),\n",
    "                                nn.Linear(150, self.num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        outputs = self.beit_model(x).last_hidden_state\n",
    "        \n",
    "        h0 = torch.zeros(self.num_layers, outputs.size(0), self.hidden_size).to(dml)\n",
    "        c0 = torch.zeros(self.num_layers, outputs.size(0), self.hidden_size).to(dml)\n",
    "        \n",
    "        out, _  = self.lstm(outputs, (h0,c0)) #out: tensor of shape (batch size, seq_length, hidden_size)\n",
    "#         print(out.shape)\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "\n",
    "input_size = 768\n",
    "hidden_size = 300\n",
    "num_layers = 2\n",
    "num_classes = 2\n",
    "\n",
    "base_model = BEiTLSTM(input_size, hidden_size, num_layers, num_classes).to(dml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['beit_model.encoder.layer.0.attention.attention.relative_position_bias.relative_position_index', 'beit_model.encoder.layer.1.attention.attention.relative_position_bias.relative_position_index', 'beit_model.encoder.layer.2.attention.attention.relative_position_bias.relative_position_index', 'beit_model.encoder.layer.3.attention.attention.relative_position_bias.relative_position_index', 'beit_model.encoder.layer.4.attention.attention.relative_position_bias.relative_position_index', 'beit_model.encoder.layer.5.attention.attention.relative_position_bias.relative_position_index', 'beit_model.encoder.layer.6.attention.attention.relative_position_bias.relative_position_index', 'beit_model.encoder.layer.7.attention.attention.relative_position_bias.relative_position_index', 'beit_model.encoder.layer.8.attention.attention.relative_position_bias.relative_position_index', 'beit_model.encoder.layer.9.attention.attention.relative_position_bias.relative_position_index', 'beit_model.encoder.layer.10.attention.attention.relative_position_bias.relative_position_index', 'beit_model.encoder.layer.11.attention.attention.relative_position_bias.relative_position_index'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"../trained-models/beitlstmallnew.pt\"\n",
    "base_model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(size = (224, 224)),\n",
    "        transforms.ToTensor()])\n",
    "\n",
    "image_processor = BeitImageProcessor.from_pretrained(\"microsoft/beit-base-patch16-224\") \n",
    "\n",
    "def my_transform(image):\n",
    "    pil_image = Image.fromarray(image)\n",
    "    pixel_values = image_processor(pil_image, return_tensors=\"pt\").pixel_values\n",
    "    return pixel_values.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(images):\n",
    "    base_model.eval()\n",
    "    batch = torch.stack(tuple(my_transform(i) for i in images), dim=0)\n",
    "    batch = batch.to(dml)\n",
    "    \n",
    "    logits = base_model(batch)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    return probs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_image(image):\n",
    "    explainer = lime_image.LimeImageExplainer()\n",
    "    explanation = explainer.explain_instance(np.array(image), \n",
    "                                            batch_predict, # classification function\n",
    "                                            hide_color=0, \n",
    "                                            batch_size=20,\n",
    "                                            random_seed=240,\n",
    "                                            num_samples=1000)\n",
    "    \n",
    "    # temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False)\n",
    "    # img_boundry1 = mark_boundaries(temp/255.0, mask)\n",
    "  \n",
    "    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=5, hide_rest=False)\n",
    "    img_boundry2 = mark_boundaries(temp/255.0, mask)\n",
    "  \n",
    "    return img_boundry2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8c61e8994a461188086ce7f62552fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6fcc3a7f7c4b5b81cda6f3158f9d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4867d573d845dab05d461e3cf50259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97352438937462aaf078fe5f35d27f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6743d8a359084662b5b0b8b5d0845b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2944efb515ed48d49d378833ad5f6fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(path)\n\u001b[0;32m      6\u001b[0m img_name \u001b[39m=\u001b[39m path\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m masked_image \u001b[39m=\u001b[39m explain_image(img)\n\u001b[0;32m      8\u001b[0m imgs\u001b[39m.\u001b[39mimsave(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(saving_path, img_name), masked_image)\n",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m, in \u001b[0;36mexplain_image\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexplain_image\u001b[39m(image):\n\u001b[0;32m      2\u001b[0m     explainer \u001b[39m=\u001b[39m lime_image\u001b[39m.\u001b[39mLimeImageExplainer()\n\u001b[1;32m----> 3\u001b[0m     explanation \u001b[39m=\u001b[39m explainer\u001b[39m.\u001b[39;49mexplain_instance(np\u001b[39m.\u001b[39;49marray(image), \n\u001b[0;32m      4\u001b[0m                                             batch_predict, \u001b[39m# classification function\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m                                             hide_color\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, \n\u001b[0;32m      6\u001b[0m                                             batch_size\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[0;32m      7\u001b[0m                                             random_seed\u001b[39m=\u001b[39;49m\u001b[39m240\u001b[39;49m,\n\u001b[0;32m      8\u001b[0m                                             num_samples\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n\u001b[0;32m     10\u001b[0m     \u001b[39m# temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[39m# img_boundry1 = mark_boundaries(temp/255.0, mask)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     temp, mask \u001b[39m=\u001b[39m explanation\u001b[39m.\u001b[39mget_image_and_mask(explanation\u001b[39m.\u001b[39mtop_labels[\u001b[39m0\u001b[39m], positive_only\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, num_features\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, hide_rest\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32me:\\Study materials\\Paper works\\Brain tumour\\BraTS\\brain-tumour\\venv\\lib\\site-packages\\lime\\lime_image.py:198\u001b[0m, in \u001b[0;36mLimeImageExplainer.explain_instance\u001b[1;34m(self, image, classifier_fn, labels, hide_color, top_labels, num_features, num_samples, batch_size, segmentation_fn, distance_metric, model_regressor, random_seed)\u001b[0m\n\u001b[0;32m    194\u001b[0m     fudged_image[:] \u001b[39m=\u001b[39m hide_color\n\u001b[0;32m    196\u001b[0m top \u001b[39m=\u001b[39m labels\n\u001b[1;32m--> 198\u001b[0m data, labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_labels(image, fudged_image, segments,\n\u001b[0;32m    199\u001b[0m                                 classifier_fn, num_samples,\n\u001b[0;32m    200\u001b[0m                                 batch_size\u001b[39m=\u001b[39;49mbatch_size)\n\u001b[0;32m    202\u001b[0m distances \u001b[39m=\u001b[39m sklearn\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mpairwise_distances(\n\u001b[0;32m    203\u001b[0m     data,\n\u001b[0;32m    204\u001b[0m     data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m    205\u001b[0m     metric\u001b[39m=\u001b[39mdistance_metric\n\u001b[0;32m    206\u001b[0m )\u001b[39m.\u001b[39mravel()\n\u001b[0;32m    208\u001b[0m ret_exp \u001b[39m=\u001b[39m ImageExplanation(image, segments)\n",
      "File \u001b[1;32me:\\Study materials\\Paper works\\Brain tumour\\BraTS\\brain-tumour\\venv\\lib\\site-packages\\lime\\lime_image.py:261\u001b[0m, in \u001b[0;36mLimeImageExplainer.data_labels\u001b[1;34m(self, image, fudged_image, segments, classifier_fn, num_samples, batch_size)\u001b[0m\n\u001b[0;32m    259\u001b[0m imgs\u001b[39m.\u001b[39mappend(temp)\n\u001b[0;32m    260\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(imgs) \u001b[39m==\u001b[39m batch_size:\n\u001b[1;32m--> 261\u001b[0m     preds \u001b[39m=\u001b[39m classifier_fn(np\u001b[39m.\u001b[39;49marray(imgs))\n\u001b[0;32m    262\u001b[0m     labels\u001b[39m.\u001b[39mextend(preds)\n\u001b[0;32m    263\u001b[0m     imgs \u001b[39m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m, in \u001b[0;36mbatch_predict\u001b[1;34m(images)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_predict\u001b[39m(images):\n\u001b[0;32m      2\u001b[0m     base_model\u001b[39m.\u001b[39meval()\n\u001b[1;32m----> 3\u001b[0m     batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(\u001b[39mtuple\u001b[39;49m(my_transform(i) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m images), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m     batch \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mto(dml)\n\u001b[0;32m      6\u001b[0m     logits \u001b[39m=\u001b[39m base_model(batch)\n",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_predict\u001b[39m(images):\n\u001b[0;32m      2\u001b[0m     base_model\u001b[39m.\u001b[39meval()\n\u001b[1;32m----> 3\u001b[0m     batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(\u001b[39mtuple\u001b[39m(my_transform(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m images), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m     batch \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mto(dml)\n\u001b[0;32m      6\u001b[0m     logits \u001b[39m=\u001b[39m base_model(batch)\n",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m, in \u001b[0;36mmy_transform\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmy_transform\u001b[39m(image):\n\u001b[0;32m      9\u001b[0m     pil_image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(image)\n\u001b[1;32m---> 10\u001b[0m     pixel_values \u001b[39m=\u001b[39m image_processor(pil_image, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mpixel_values\n\u001b[0;32m     11\u001b[0m     \u001b[39mreturn\u001b[39;00m pixel_values\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[1;32me:\\Study materials\\Paper works\\Brain tumour\\BraTS\\brain-tumour\\venv\\lib\\site-packages\\transformers\\models\\beit\\image_processing_beit.py:350\u001b[0m, in \u001b[0;36mBeitImageProcessor.__call__\u001b[1;34m(self, images, segmentation_maps, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, images, segmentation_maps\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    348\u001b[0m     \u001b[39m# Overrides the `__call__` method of the `Preprocessor` class such that the images and segmentation maps can both\u001b[39;00m\n\u001b[0;32m    349\u001b[0m     \u001b[39m# be passed in as positional arguments.\u001b[39;00m\n\u001b[1;32m--> 350\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(images, segmentation_maps\u001b[39m=\u001b[39msegmentation_maps, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Study materials\\Paper works\\Brain tumour\\BraTS\\brain-tumour\\venv\\lib\\site-packages\\transformers\\image_processing_utils.py:494\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[1;34m(self, images, **kwargs)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, images, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchFeature:\n\u001b[0;32m    493\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 494\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(images, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Study materials\\Paper works\\Brain tumour\\BraTS\\brain-tumour\\venv\\lib\\site-packages\\transformers\\models\\beit\\image_processing_beit.py:457\u001b[0m, in \u001b[0;36mBeitImageProcessor.preprocess\u001b[1;34m(self, images, segmentation_maps, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_reduce_labels, return_tensors, data_format, **kwargs)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[39mif\u001b[39;00m do_normalize \u001b[39mand\u001b[39;00m (image_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m image_std \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    455\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImage mean and std must be specified if do_normalize is True.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 457\u001b[0m images \u001b[39m=\u001b[39m [\n\u001b[0;32m    458\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_image(\n\u001b[0;32m    459\u001b[0m         image\u001b[39m=\u001b[39mimg,\n\u001b[0;32m    460\u001b[0m         do_resize\u001b[39m=\u001b[39mdo_resize,\n\u001b[0;32m    461\u001b[0m         do_center_crop\u001b[39m=\u001b[39mdo_center_crop,\n\u001b[0;32m    462\u001b[0m         do_rescale\u001b[39m=\u001b[39mdo_rescale,\n\u001b[0;32m    463\u001b[0m         do_normalize\u001b[39m=\u001b[39mdo_normalize,\n\u001b[0;32m    464\u001b[0m         resample\u001b[39m=\u001b[39mresample,\n\u001b[0;32m    465\u001b[0m         size\u001b[39m=\u001b[39msize,\n\u001b[0;32m    466\u001b[0m         rescale_factor\u001b[39m=\u001b[39mrescale_factor,\n\u001b[0;32m    467\u001b[0m         crop_size\u001b[39m=\u001b[39mcrop_size,\n\u001b[0;32m    468\u001b[0m         image_mean\u001b[39m=\u001b[39mimage_mean,\n\u001b[0;32m    469\u001b[0m         image_std\u001b[39m=\u001b[39mimage_std,\n\u001b[0;32m    470\u001b[0m         data_format\u001b[39m=\u001b[39mdata_format,\n\u001b[0;32m    471\u001b[0m     )\n\u001b[0;32m    472\u001b[0m     \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m images\n\u001b[0;32m    473\u001b[0m ]\n\u001b[0;32m    475\u001b[0m data \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m: images}\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m segmentation_maps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Study materials\\Paper works\\Brain tumour\\BraTS\\brain-tumour\\venv\\lib\\site-packages\\transformers\\models\\beit\\image_processing_beit.py:458\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[39mif\u001b[39;00m do_normalize \u001b[39mand\u001b[39;00m (image_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m image_std \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    455\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImage mean and std must be specified if do_normalize is True.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    457\u001b[0m images \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 458\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preprocess_image(\n\u001b[0;32m    459\u001b[0m         image\u001b[39m=\u001b[39;49mimg,\n\u001b[0;32m    460\u001b[0m         do_resize\u001b[39m=\u001b[39;49mdo_resize,\n\u001b[0;32m    461\u001b[0m         do_center_crop\u001b[39m=\u001b[39;49mdo_center_crop,\n\u001b[0;32m    462\u001b[0m         do_rescale\u001b[39m=\u001b[39;49mdo_rescale,\n\u001b[0;32m    463\u001b[0m         do_normalize\u001b[39m=\u001b[39;49mdo_normalize,\n\u001b[0;32m    464\u001b[0m         resample\u001b[39m=\u001b[39;49mresample,\n\u001b[0;32m    465\u001b[0m         size\u001b[39m=\u001b[39;49msize,\n\u001b[0;32m    466\u001b[0m         rescale_factor\u001b[39m=\u001b[39;49mrescale_factor,\n\u001b[0;32m    467\u001b[0m         crop_size\u001b[39m=\u001b[39;49mcrop_size,\n\u001b[0;32m    468\u001b[0m         image_mean\u001b[39m=\u001b[39;49mimage_mean,\n\u001b[0;32m    469\u001b[0m         image_std\u001b[39m=\u001b[39;49mimage_std,\n\u001b[0;32m    470\u001b[0m         data_format\u001b[39m=\u001b[39;49mdata_format,\n\u001b[0;32m    471\u001b[0m     )\n\u001b[0;32m    472\u001b[0m     \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m images\n\u001b[0;32m    473\u001b[0m ]\n\u001b[0;32m    475\u001b[0m data \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m: images}\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m segmentation_maps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Study materials\\Paper works\\Brain tumour\\BraTS\\brain-tumour\\venv\\lib\\site-packages\\transformers\\models\\beit\\image_processing_beit.py:293\u001b[0m, in \u001b[0;36mBeitImageProcessor._preprocess_image\u001b[1;34m(self, image, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, data_format)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[39m# All transformations expect numpy arrays.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m image \u001b[39m=\u001b[39m to_numpy_array(image)\n\u001b[1;32m--> 293\u001b[0m image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preprocess(\n\u001b[0;32m    294\u001b[0m     image,\n\u001b[0;32m    295\u001b[0m     do_reduce_labels\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    296\u001b[0m     do_resize\u001b[39m=\u001b[39;49mdo_resize,\n\u001b[0;32m    297\u001b[0m     size\u001b[39m=\u001b[39;49msize,\n\u001b[0;32m    298\u001b[0m     resample\u001b[39m=\u001b[39;49mresample,\n\u001b[0;32m    299\u001b[0m     do_center_crop\u001b[39m=\u001b[39;49mdo_center_crop,\n\u001b[0;32m    300\u001b[0m     crop_size\u001b[39m=\u001b[39;49mcrop_size,\n\u001b[0;32m    301\u001b[0m     do_rescale\u001b[39m=\u001b[39;49mdo_rescale,\n\u001b[0;32m    302\u001b[0m     rescale_factor\u001b[39m=\u001b[39;49mrescale_factor,\n\u001b[0;32m    303\u001b[0m     do_normalize\u001b[39m=\u001b[39;49mdo_normalize,\n\u001b[0;32m    304\u001b[0m     image_mean\u001b[39m=\u001b[39;49mimage_mean,\n\u001b[0;32m    305\u001b[0m     image_std\u001b[39m=\u001b[39;49mimage_std,\n\u001b[0;32m    306\u001b[0m )\n\u001b[0;32m    307\u001b[0m \u001b[39mif\u001b[39;00m data_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    308\u001b[0m     image \u001b[39m=\u001b[39m to_channel_dimension_format(image, data_format)\n",
      "File \u001b[1;32me:\\Study materials\\Paper works\\Brain tumour\\BraTS\\brain-tumour\\venv\\lib\\site-packages\\transformers\\models\\beit\\image_processing_beit.py:262\u001b[0m, in \u001b[0;36mBeitImageProcessor._preprocess\u001b[1;34m(self, image, do_reduce_labels, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std)\u001b[0m\n\u001b[0;32m    259\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreduce_label(image)\n\u001b[0;32m    261\u001b[0m \u001b[39mif\u001b[39;00m do_resize:\n\u001b[1;32m--> 262\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresize(image\u001b[39m=\u001b[39;49mimage, size\u001b[39m=\u001b[39;49msize, resample\u001b[39m=\u001b[39;49mresample)\n\u001b[0;32m    264\u001b[0m \u001b[39mif\u001b[39;00m do_center_crop:\n\u001b[0;32m    265\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcenter_crop(image\u001b[39m=\u001b[39mimage, size\u001b[39m=\u001b[39mcrop_size)\n",
      "File \u001b[1;32me:\\Study materials\\Paper works\\Brain tumour\\BraTS\\brain-tumour\\venv\\lib\\site-packages\\transformers\\models\\beit\\image_processing_beit.py:166\u001b[0m, in \u001b[0;36mBeitImageProcessor.resize\u001b[1;34m(self, image, size, resample, data_format, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mheight\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m size \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m size:\n\u001b[0;32m    165\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe `size` argument must contain `height` and `width` keys. Got \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 166\u001b[0m \u001b[39mreturn\u001b[39;00m resize(\n\u001b[0;32m    167\u001b[0m     image, size\u001b[39m=\u001b[39m(size[\u001b[39m\"\u001b[39m\u001b[39mheight\u001b[39m\u001b[39m\"\u001b[39m], size[\u001b[39m\"\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m\"\u001b[39m]), resample\u001b[39m=\u001b[39mresample, data_format\u001b[39m=\u001b[39mdata_format, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    168\u001b[0m )\n",
      "File \u001b[1;32me:\\Study materials\\Paper works\\Brain tumour\\BraTS\\brain-tumour\\venv\\lib\\site-packages\\transformers\\image_transforms.py:306\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(image, size, resample, reducing_gap, data_format, return_numpy)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(image, PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mImage):\n\u001b[0;32m    305\u001b[0m     do_rescale \u001b[39m=\u001b[39m _rescale_for_pil_conversion(image)\n\u001b[1;32m--> 306\u001b[0m     image \u001b[39m=\u001b[39m to_pil_image(image, do_rescale\u001b[39m=\u001b[39;49mdo_rescale)\n\u001b[0;32m    307\u001b[0m height, width \u001b[39m=\u001b[39m size\n\u001b[0;32m    308\u001b[0m \u001b[39m# PIL images are in the format (width, height)\u001b[39;00m\n",
      "File \u001b[1;32me:\\Study materials\\Paper works\\Brain tumour\\BraTS\\brain-tumour\\venv\\lib\\site-packages\\transformers\\image_transforms.py:166\u001b[0m, in \u001b[0;36mto_pil_image\u001b[1;34m(image, do_rescale)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_pil_image\u001b[39m(\n\u001b[0;32m    148\u001b[0m     image: Union[np\u001b[39m.\u001b[39mndarray, \u001b[39m\"\u001b[39m\u001b[39mPIL.Image.Image\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtorch.Tensor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtf.Tensor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mjnp.ndarray\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m    149\u001b[0m     do_rescale: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    150\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPIL.Image.Image\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    151\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[39m    Converts `image` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[39m    needed.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39m        `PIL.Image.Image`: The converted image.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m     requires_backends(to_pil_image, [\u001b[39m\"\u001b[39;49m\u001b[39mvision\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    168\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(image, PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mImage):\n\u001b[0;32m    169\u001b[0m         \u001b[39mreturn\u001b[39;00m image\n",
      "File \u001b[1;32me:\\Study materials\\Paper works\\Brain tumour\\BraTS\\brain-tumour\\venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1025\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(TF_IMPORT_ERROR_WITH_PYTORCH\u001b[39m.\u001b[39mformat(name))\n\u001b[0;32m   1024\u001b[0m checks \u001b[39m=\u001b[39m (BACKENDS_MAPPING[backend] \u001b[39mfor\u001b[39;00m backend \u001b[39min\u001b[39;00m backends)\n\u001b[1;32m-> 1025\u001b[0m failed \u001b[39m=\u001b[39m [msg\u001b[39m.\u001b[39mformat(name) \u001b[39mfor\u001b[39;00m available, msg \u001b[39min\u001b[39;00m checks \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m available()]\n\u001b[0;32m   1026\u001b[0m \u001b[39mif\u001b[39;00m failed:\n\u001b[0;32m   1027\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(failed))\n",
      "File \u001b[1;32me:\\Study materials\\Paper works\\Brain tumour\\BraTS\\brain-tumour\\venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1025\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(TF_IMPORT_ERROR_WITH_PYTORCH\u001b[39m.\u001b[39mformat(name))\n\u001b[0;32m   1024\u001b[0m checks \u001b[39m=\u001b[39m (BACKENDS_MAPPING[backend] \u001b[39mfor\u001b[39;00m backend \u001b[39min\u001b[39;00m backends)\n\u001b[1;32m-> 1025\u001b[0m failed \u001b[39m=\u001b[39m [msg\u001b[39m.\u001b[39mformat(name) \u001b[39mfor\u001b[39;00m available, msg \u001b[39min\u001b[39;00m checks \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m available()]\n\u001b[0;32m   1026\u001b[0m \u001b[39mif\u001b[39;00m failed:\n\u001b[0;32m   1027\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(failed))\n",
      "File \u001b[1;32me:\\Study materials\\Paper works\\Brain tumour\\BraTS\\brain-tumour\\venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:547\u001b[0m, in \u001b[0;36mis_vision_available\u001b[1;34m()\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[39mif\u001b[39;00m _pil_available:\n\u001b[0;32m    546\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 547\u001b[0m         package_version \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mmetadata\u001b[39m.\u001b[39;49mversion(\u001b[39m\"\u001b[39;49m\u001b[39mPillow\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    548\u001b[0m     \u001b[39mexcept\u001b[39;00m importlib\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mPackageNotFoundError:\n\u001b[0;32m    549\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\metadata\\__init__.py:984\u001b[0m, in \u001b[0;36mversion\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    977\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mversion\u001b[39m(distribution_name):\n\u001b[0;32m    978\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \n\u001b[0;32m    980\u001b[0m \u001b[39m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \u001b[39m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[0;32m    982\u001b[0m \u001b[39m        \"Version\" metadata key.\u001b[39;00m\n\u001b[0;32m    983\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 984\u001b[0m     \u001b[39mreturn\u001b[39;00m distribution(distribution_name)\u001b[39m.\u001b[39mversion\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\metadata\\__init__.py:957\u001b[0m, in \u001b[0;36mdistribution\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdistribution\u001b[39m(distribution_name):\n\u001b[0;32m    952\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \n\u001b[0;32m    954\u001b[0m \u001b[39m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[0;32m    955\u001b[0m \u001b[39m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[0;32m    956\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 957\u001b[0m     \u001b[39mreturn\u001b[39;00m Distribution\u001b[39m.\u001b[39;49mfrom_name(distribution_name)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\metadata\\__init__.py:544\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[39mfor\u001b[39;00m resolver \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_discover_resolvers():\n\u001b[0;32m    543\u001b[0m     dists \u001b[39m=\u001b[39m resolver(DistributionFinder\u001b[39m.\u001b[39mContext(name\u001b[39m=\u001b[39mname))\n\u001b[1;32m--> 544\u001b[0m     dist \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(dists), \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    545\u001b[0m     \u001b[39mif\u001b[39;00m dist \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    546\u001b[0m         \u001b[39mreturn\u001b[39;00m dist\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\metadata\\__init__.py:904\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Find metadata directories in paths heuristically.\"\"\"\u001b[39;00m\n\u001b[0;32m    902\u001b[0m prepared \u001b[39m=\u001b[39m Prepared(name)\n\u001b[0;32m    903\u001b[0m \u001b[39mreturn\u001b[39;00m itertools\u001b[39m.\u001b[39mchain\u001b[39m.\u001b[39mfrom_iterable(\n\u001b[1;32m--> 904\u001b[0m     path\u001b[39m.\u001b[39;49msearch(prepared) \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m \u001b[39mmap\u001b[39m(FastPath, paths)\n\u001b[0;32m    905\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\metadata\\__init__.py:802\u001b[0m, in \u001b[0;36mFastPath.search\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msearch\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[1;32m--> 802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlookup(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmtime)\u001b[39m.\u001b[39msearch(name)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\metadata\\__init__.py:807\u001b[0m, in \u001b[0;36mFastPath.mtime\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    805\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmtime\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    806\u001b[0m     \u001b[39mwith\u001b[39;00m suppress(\u001b[39mOSError\u001b[39;00m):\n\u001b[1;32m--> 807\u001b[0m         \u001b[39mreturn\u001b[39;00m os\u001b[39m.\u001b[39;49mstat(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot)\u001b[39m.\u001b[39mst_mtime\n\u001b[0;32m    808\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlookup\u001b[39m.\u001b[39mcache_clear()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "image_path = glob.glob(\"../images/misclassified-new/beitlstm/*\")\n",
    "saving_path = \"../images/lime/beit-lstm\"\n",
    "\n",
    "for path in image_path:\n",
    "    img = cv2.imread(path)\n",
    "    img_name = path.split(\"\\\\\")[-1]\n",
    "    masked_image = explain_image(img)\n",
    "    imgs.imsave(os.path.join(saving_path, img_name), masked_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "total_img = len(image_path)\n",
    "rem_img = total_img % batch_size\n",
    "predictions = []\n",
    "\n",
    "r = batch_size\n",
    "l = 0\n",
    "for i in range(total_img // batch_size):\n",
    "    l = i*batch_size\n",
    "    r = i*batch_size + batch_size\n",
    "    batch = [cv2.imread(img) for img in image_path[l:r]]\n",
    "    predictions.extend(np.argmax(batch_predict(batch), axis=1))\n",
    "\n",
    "batch = [cv2.imread(img) for img in image_path[r:]]\n",
    "predictions.extend(np.argmax(batch_predict(batch), axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
